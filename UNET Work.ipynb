{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1590ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "#from albumentations import HorizontalFlip, VerticalFlip, ElasticTransform, GridDistortion, OpticalDistortion, CoarseDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef60d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mukesh\\active-learning\\CEAL-Medical-Image-Segmentation\\src\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(\"active-learning/CEAL-Medical-Image-Segmentation/src\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb955e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2000 - 2000\n",
      "Test: 600 - 600\n"
     ]
    }
   ],
   "source": [
    "H = 192\n",
    "W = 256\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\" X = Images and Y = masks \"\"\"\n",
    "\n",
    "    X_train = sorted(glob(os.path.join(path, \"trainx\", \"*.jpg\")))\n",
    "    y_train = sorted(glob(os.path.join(path, \"trainy\", \"*.jpg\")))\n",
    "\n",
    "    test_x = sorted(glob(os.path.join(path, \"testx\", \"*.jpg\")))\n",
    "    test_y = sorted(glob(os.path.join(path, \"testy\", \"*.jpg\")))\n",
    "\n",
    "    return (X_train, y_train), (test_x, test_y)\n",
    "\n",
    "def augment_data(images, masks, save_path, augment=True):\n",
    "    \n",
    "\n",
    "    for idx, (x, y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
    "        \"\"\" Extracting names \"\"\"\n",
    "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        \"\"\" Reading image and mask \"\"\"\n",
    "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        y = imageio.mimread(y)[0]\n",
    "\n",
    "        if augment == True:\n",
    "            aug = HorizontalFlip(p=1.0)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x1 = augmented[\"image\"]\n",
    "            y1 = augmented[\"mask\"]\n",
    "\n",
    "            aug = VerticalFlip(p=1.0)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x2 = augmented[\"image\"]\n",
    "            y2 = augmented[\"mask\"]\n",
    "\n",
    "            aug = ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x3 = augmented['image']\n",
    "            y3 = augmented['mask']\n",
    "\n",
    "            aug = GridDistortion(p=1)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x4 = augmented['image']\n",
    "            y4 = augmented['mask']\n",
    "\n",
    "            aug = OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x5 = augmented['image']\n",
    "            y5 = augmented['mask']\n",
    "\n",
    "            X = [x, x1, x2, x3, x4, x5]\n",
    "            Y = [y, y1, y2, y3, y4, y5]\n",
    "\n",
    "        else:\n",
    "            X = [x]\n",
    "            Y = [y]\n",
    "\n",
    "        index = 0\n",
    "        for i, m in zip(X, Y):\n",
    "            i = cv2.resize(i, (W, H))\n",
    "            m = cv2.resize(m, (W, H))\n",
    "\n",
    "            if len(X) == 1:\n",
    "                tmp_image_name = f\"{name}.jpg\"\n",
    "                tmp_mask_name = f\"{name}.jpg\"\n",
    "            else:\n",
    "                tmp_image_name = f\"{name}_{index}.jpg\"\n",
    "                tmp_mask_name = f\"{name}_{index}.jpg\"\n",
    "\n",
    "            image_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
    "            mask_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
    "\n",
    "            cv2.imwrite(image_path, i)\n",
    "            cv2.imwrite(mask_path, m)\n",
    "\n",
    "            index += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Seeding \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    \"\"\" Load the data \"\"\"\n",
    "    data_path = \"archive/\"\n",
    "    (X_train, y_train), (test_x, test_y) = load_data(data_path)\n",
    "\n",
    "    print(f\"Train: {len(X_train)} - {len(y_train)}\")\n",
    "    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
    "\n",
    "    \"\"\" Creating directories \"\"\"\n",
    "    create_dir(\"new_data/train/image\")\n",
    "    create_dir(\"new_data/train/mask\")\n",
    "    create_dir(\"new_data/test/image\")\n",
    "    create_dir(\"new_data/test/mask\")\n",
    "\n",
    "    #augment_data(X_train, y_train, \"new_data/train/\", augment=False)\n",
    "    #augment_data(test_x, test_y, \"new_data/test/\", augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "183f7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    # x = cv2.resize(x, (W, H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_mask(path):\n",
    "    path = path.decode()\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  ## (512, 512)\n",
    "    # x = cv2.resize(x, (W, H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.expand_dims(x, axis=-1)              ## (512, 512, 1)\n",
    "    return x\n",
    "\n",
    "def tf_parse(x, y):\n",
    "    def _parse(x, y):\n",
    "        x = read_image(x)\n",
    "        y = read_mask(y)\n",
    "        return x, y\n",
    "\n",
    "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
    "    x.set_shape([H, W, 3])\n",
    "    y.set_shape([H, W, 1])\n",
    "    return x, y\n",
    "\n",
    "def tf_dataset(X, Y, batch_size=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(4)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "805b5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model):\n",
    "    \"\"\"\n",
    "    Data prediction for a given model\n",
    "    :param data: input data to predict.\n",
    "    :param model: unet model.\n",
    "    :return: predictions.\n",
    "    \"\"\"\n",
    "    return model.predict(data, verbose=0)\n",
    "\n",
    "\n",
    "def compute_uncertain(sample, prediction, model):\n",
    "    \"\"\"\n",
    "    Computes uncertainty map for a given sample and its prediction for a given model, based on the\n",
    "    number of step predictions defined in constants file.\n",
    "    :param sample: input sample.\n",
    "    :param prediction: input sample prediction.\n",
    "    :param model: unet model with Dropout layers.\n",
    "    :return: uncertainty map.\n",
    "    \"\"\"\n",
    "    X = np.zeros([1, img_rows, img_cols])\n",
    "\n",
    "    for t in range(nb_step_predictions):\n",
    "        prediction = model.predict(sample, verbose=0).reshape([1, img_rows, img_cols])\n",
    "        X = np.concatenate((X, prediction))\n",
    "\n",
    "    X = np.delete(X, [0], 0)\n",
    "\n",
    "    if (apply_edt):\n",
    "        # apply distance transform normalization.\n",
    "        var = np.var(X, axis=0)\n",
    "        transform = range_transform(edt(prediction))\n",
    "        return np.sum(var * transform)\n",
    "\n",
    "    else:\n",
    "        return np.sum(np.var(X, axis=0))\n",
    "\n",
    "\n",
    "def interval(data, start, end):\n",
    "    \"\"\"\n",
    "    Returns the index of data within range values from start to end.\n",
    "    :param data: numpy array of data.\n",
    "    :param start: starting value.\n",
    "    :param end: ending value.\n",
    "    :return: numpy array of data index.\n",
    "    \"\"\"\n",
    "    p = np.where(data >= start)[0]\n",
    "    return p[np.where(data[p] < end)[0]]\n",
    "\n",
    "\n",
    "def get_pseudo_index(uncertain, nb_pseudo):\n",
    "    \"\"\"\n",
    "    Gives the index of the most certain data, to make the pseudo annotations.\n",
    "    :param uncertain: Numpy array with the overall uncertainty values of the unlabeled data.\n",
    "    :param nb_pseudo: Total of pseudo samples.\n",
    "    :return: Numpy array of index.\n",
    "    \"\"\"\n",
    "    h = np.histogram(uncertain, 80)\n",
    "\n",
    "    pseudo = interval(uncertain, h[1][np.argmax(h[0])], h[1][np.argmax(h[0]) + 1])\n",
    "    np.random.shuffle(pseudo)\n",
    "    return pseudo[0:nb_pseudo]\n",
    "\n",
    "\n",
    "def random_index(uncertain, nb_random):\n",
    "    \"\"\"\n",
    "    Gives the index of the random selection to be manually annotated.\n",
    "    :param uncertain: Numpy array with the overall uncertainty values of the unlabeled data.\n",
    "    :param nb_random: Total of random samples.\n",
    "    :return: Numpy array of index.\n",
    "    \"\"\"\n",
    "    histo = np.histogram(uncertain, 80)\n",
    "    # TODO: automatic selection of random range\n",
    "    index = interval(uncertain, histo[1][np.argmax(histo[0]) + 6], histo[1][len(histo[0]) - 33])\n",
    "    np.random.shuffle(index)\n",
    "    return index[0:nb_random]\n",
    "\n",
    "\n",
    "def no_detections_index(uncertain, nb_no_detections):\n",
    "    \"\"\"\n",
    "    Gives the index of the no detected samples to be manually annotated.\n",
    "    :param uncertain: Numpy array with the overall uncertainty values of the unlabeled data.\n",
    "    :param nb_no_detections: Total of no detected samples.\n",
    "    :return: Numpy array of index.\n",
    "    \"\"\"\n",
    "    return np.argsort(uncertain)[0:nb_no_detections]\n",
    "\n",
    "\n",
    "def most_uncertain_index(uncertain, nb_most_uncertain, rate):\n",
    "    \"\"\"\n",
    "     Gives the index of the most uncertain samples to be manually annotated.\n",
    "    :param uncertain: Numpy array with the overall uncertainty values of the unlabeled data.\n",
    "    :param nb_most_uncertain: Total of most uncertain samples.\n",
    "    :param rate: Hash threshold to define the most uncertain area. Bin of uncertainty histogram.\n",
    "    TODO: automatic selection of rate.\n",
    "    :return: Numpy array of index.\n",
    "    \"\"\"\n",
    "    data = np.array([]).astype('int')\n",
    "\n",
    "    histo = np.histogram(uncertain, 80)\n",
    "\n",
    "    p = np.arange(len(histo[0]) - rate, len(histo[0]))  # index of last bins above the rate\n",
    "    pr = np.argsort(histo[0][p])  # p index accendent sorted\n",
    "    cnt = 0\n",
    "    pos = 0\n",
    "    index = np.array([]).astype('int')\n",
    "\n",
    "    while (cnt < nb_most_uncertain and pos < len(pr)):\n",
    "        sbin = histo[0][p[pr[pos]]]\n",
    "\n",
    "        index = np.append(index, p[pr[pos]])\n",
    "        cnt = cnt + sbin\n",
    "        pos = pos + 1\n",
    "\n",
    "    for i in range(0, pos):\n",
    "        data = np.concatenate((data, interval(uncertain, histo[1][index[i]], histo[1][index[i] + 1])))\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    return data[0:nb_most_uncertain]\n",
    "\n",
    "\n",
    "def get_oracle_index(uncertain, nb_no_detections, nb_random, nb_most_uncertain, rate):\n",
    "    \"\"\"\n",
    "    Gives the index of the unlabeled data to annotated at specific CEAL iteration, based on their uncertainty.\n",
    "    :param uncertain: Numpy array with the overall uncertainty values of the unlabeled data.\n",
    "    :param nb_no_detections: Total of no detected samples.\n",
    "    :param nb_random: Total of random samples.\n",
    "    :param nb_most_uncertain: Total of most uncertain samples.\n",
    "    :param rate: Hash threshold to define the most uncertain area. Bin of uncertainty histogram.\n",
    "    :return: Numpy array of index.\n",
    "    \"\"\"\n",
    "    return np.concatenate((no_detections_index(uncertain, nb_no_detections), random_index(uncertain, nb_random),\n",
    "                           most_uncertain_index(uncertain, nb_most_uncertain, rate)))\n",
    "\n",
    "\n",
    "def compute_dice_coef(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Dice-Coefficient of a prediction given its ground truth.\n",
    "    :param y_true: Ground truth.\n",
    "    :param y_pred: Prediction.\n",
    "    :return: Dice-Coefficient value.\n",
    "    \"\"\"\n",
    "    smooth = 1.  # smoothing value to deal zero denominators.\n",
    "    y_true_f = y_true.reshape([1, img_rows * img_cols])\n",
    "    y_pred_f = y_pred.reshape([1, img_rows * img_cols])\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def compute_train_sets(X_train, y_train, labeled_index, unlabeled_index, weights, iteration):\n",
    "    \"\"\"\n",
    "    Performs the Cost-Effective Active Learning labeling step, giving the available training data for each iteration.\n",
    "    :param X_train: Overall training data.\n",
    "    :param y_train: Overall training labels. Including the unlabeled samples to simulate the oracle annotations.\n",
    "    :param labeled_index: Index of labeled samples.\n",
    "    :param unlabeled_index: Index of unlabeled samples.\n",
    "    :param weights: pre-trained unet weights.\n",
    "    :param iteration: Currently CEAL iteration.\n",
    "\n",
    "    :return: X_labeled_train: Update of labeled training data, adding the manual and pseudo annotations.\n",
    "    :return: y_labeled_train: Update of labeled training labels, adding the manual and pseudo annotations.\n",
    "    :return: labeled_index: Update of labeled index, adding the manual annotations.\n",
    "    :return: unlabeled_index: Update of labeled index, removing the manual annotations.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\nActive iteration \" + str(iteration))\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    # load models\n",
    "    modelUncertain = get_unet(dropout=True)\n",
    "    modelUncertain.load_weights(weights)\n",
    "    modelPredictions = get_unet(dropout=False)\n",
    "    modelPredictions.load_weights(weights)\n",
    "\n",
    "    # predictions\n",
    "    print(\"Computing log predictions ...\\n\")\n",
    "    predictions = predict(X_train[unlabeled_index], modelPredictions)\n",
    "\n",
    "    uncertain = np.zeros(len(unlabeled_index))\n",
    "    accuracy = np.zeros(len(unlabeled_index))\n",
    "\n",
    "    print(\"Computing train sets ...\")\n",
    "    for index in range(0, len(unlabeled_index)):\n",
    "\n",
    "        if index % 100 == 0:\n",
    "            print(\"completed: \" + str(index) + \"/\" + str(len(unlabeled_index)))\n",
    "\n",
    "        sample = X_train[unlabeled_index[index]].reshape([1, 1, img_rows, img_cols])\n",
    "\n",
    "        sample_prediction = cv2.threshold(predictions[index], 0.5, 1, cv2.THRESH_BINARY)[1].astype('uint8')\n",
    "\n",
    "        accuracy[index] = compute_dice_coef(y_train[unlabeled_index[index]][0], sample_prediction)\n",
    "        uncertain[index] = compute_uncertain(sample, sample_prediction, modelUncertain)\n",
    "\n",
    "    np.save(global_path + \"logs/uncertain\" + str(iteration), uncertain)\n",
    "    np.save(global_path + \"logs/accuracy\" + str(iteration), accuracy)\n",
    "\n",
    "    oracle_index = get_oracle_index(uncertain, nb_no_detections, nb_random, nb_most_uncertain,\n",
    "                                    most_uncertain_rate)\n",
    "\n",
    "    oracle_rank = unlabeled_index[oracle_index]\n",
    "\n",
    "    np.save(global_path + \"ranks/oracle\" + str(iteration), oracle_rank)\n",
    "    np.save(global_path + \"ranks/oraclelogs\" + str(iteration), oracle_index)\n",
    "\n",
    "    labeled_index = np.concatenate((labeled_index, oracle_rank))\n",
    "\n",
    "    if (iteration >= pseudo_epoch):\n",
    "\n",
    "        pseudo_index = get_pseudo_index(uncertain, nb_pseudo_initial + (pseudo_rate * (iteration - pseudo_epoch)))\n",
    "        pseudo_rank = unlabeled_index[pseudo_index]\n",
    "\n",
    "        np.save(global_path + \"ranks/pseudo\" + str(iteration), pseudo_rank)\n",
    "        np.save(global_path + \"ranks/pseudologs\" + str(iteration), pseudo_index)\n",
    "\n",
    "        X_labeled_train = np.concatenate((X_train[labeled_index], X_train[pseudo_index]))\n",
    "        y_labeled_train = np.concatenate((y_train[labeled_index], predictions[pseudo_index]))\n",
    "\n",
    "    else:\n",
    "        X_labeled_train = np.concatenate((X_train[labeled_index])).reshape([len(labeled_index), 1, img_rows, img_cols])\n",
    "        y_labeled_train = np.concatenate((y_train[labeled_index])).reshape([len(labeled_index), 1, img_rows, img_cols])\n",
    "\n",
    "    unlabeled_index = np.delete(unlabeled_index, oracle_index, 0)\n",
    "\n",
    "    return X_labeled_train, y_labeled_train, labeled_index, unlabeled_index\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    :return: Keras data generator. Data augmentation parameters.\n",
    "    \"\"\"\n",
    "    return ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        width_shift_range=0.2,\n",
    "        rotation_range=40,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "def log(history, step, log_file):\n",
    "    \"\"\"\n",
    "    Writes the training history to the log file.\n",
    "    :param history: Training history. Dictionary with training and validation scores.\n",
    "    :param step: Training step\n",
    "    :param log_file: Log file.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(history.history[\"loss\"])):\n",
    "        if len(history.history.keys()) == 4:\n",
    "            log_file.write('{0} {1} {2} {3} \\n'.format(str(step), str(i), str(history.history[\"loss\"][i]),\n",
    "                                                       str(history.history[\"val_dice_coef\"][i])))\n",
    "\n",
    "\n",
    "def create_paths():\n",
    "    \"\"\"\n",
    "    Creates all the output paths.\n",
    "    \"\"\"\n",
    "    path_ranks = global_path + \"ranks/\"\n",
    "    path_logs = global_path + \"logs/\"\n",
    "    path_plots = global_path + \"plots/\"\n",
    "    path_models = global_path + \"models/\"\n",
    "\n",
    "    if not os.path.exists(path_ranks):\n",
    "        os.makedirs(path_ranks)\n",
    "        print(\"Path created: \", path_ranks)\n",
    "\n",
    "    if not os.path.exists(path_logs):\n",
    "        os.makedirs(path_logs)\n",
    "        print(\"Path created: \", path_logs)\n",
    "\n",
    "    if not os.path.exists(path_plots):\n",
    "        os.makedirs(path_plots)\n",
    "        print(\"Path created: \", path_plots)\n",
    "\n",
    "    if not os.path.exists(path_models):\n",
    "        os.makedirs(path_models)\n",
    "        print(\"Path created: \", path_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83fb975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, merge, Conv2D, MaxPool2D, UpSampling2D, Dropout, MaxPooling2D, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend\n",
    "\n",
    "from constants import img_rows, img_cols\n",
    "\n",
    "K.image_data_format()  # Theano dimension ordering in this code\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "#Override Dropout. Make it able at test time.\n",
    "def call(self, inputs, training=None):\n",
    "    if 0. < self.rate < 1.:\n",
    "        noise_shape = self._get_noise_shape(inputs)\n",
    "        def dropped_inputs():\n",
    "            return K.dropout(inputs, self.rate, noise_shape,\n",
    "                             seed=self.seed)\n",
    "        if (training):\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "        else:\n",
    "            return K.in_test_phase(dropped_inputs, inputs, training=None)\n",
    "    return inputs\n",
    "\n",
    "Dropout.call = call\n",
    "\n",
    "def get_unet(dropout):\n",
    "    inputs = Input((img_rows, img_cols, 1))\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    \n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "    outputs = conv10\n",
    "    model = keras.models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e4663d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-89c73fd40eb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         history = model.fit(X_train[labeled_index], y_train[labeled_index], batch_size=32, nb_epoch=nb_initial_epochs,\n\u001b[0m\u001b[0;32m     33\u001b[0m                             verbose=1, shuffle=True, callbacks=[model_checkpoint])\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle\n",
    "from data import load_train_data\n",
    "from utils import *\n",
    "\n",
    "create_paths()\n",
    "log_file = open(global_path + \"logs/log_file.txt\", 'a')\n",
    "\n",
    "# CEAL data definition\n",
    "X_train, y_train = load_train_data()\n",
    "labeled_index = np.arange(0, nb_labeled)\n",
    "unlabeled_index = np.arange(nb_labeled, len(X_train))\n",
    "print(len(X_train))\n",
    "\n",
    "# (1) Initialize model\n",
    "model = get_unet(dropout=True)\n",
    "\n",
    "if initial_train:\n",
    "    model_checkpoint = ModelCheckpoint(initial_weights_path, monitor='loss', save_best_only=True)\n",
    "\n",
    "    if apply_augmentation:\n",
    "        for initial_epoch in range(0, nb_initial_epochs):\n",
    "            history = model.fit_generator(\n",
    "                data_generator().flow(X_train[labeled_index], y_train[labeled_index], batch_size=32, shuffle=True),\n",
    "                steps_per_epoch=len(labeled_index), nb_epoch=1, verbose=1, callbacks=[model_checkpoint])\n",
    "\n",
    "            model.save(initial_weights_path)\n",
    "            log(history, initial_epoch, log_file)\n",
    "    else:\n",
    "        history = model.fit(X_train[labeled_index], y_train[labeled_index], batch_size=32, nb_epoch=nb_initial_epochs,\n",
    "                            verbose=1, shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "        log(history, 0, log_file)\n",
    "else:\n",
    "    model.load_weights(initial_weights_path)\n",
    "\n",
    "# Active loop\n",
    "model_checkpoint = ModelCheckpoint(final_weights_path, monitor='loss', save_best_only=True)\n",
    "\n",
    "for iteration in range(1, nb_iterations + 1):\n",
    "    if iteration == 1:\n",
    "        weights = initial_weights_path\n",
    "\n",
    "    else:\n",
    "        weights = final_weights_path\n",
    "\n",
    "    # (2) Labeling\n",
    "    X_labeled_train, y_labeled_train, labeled_index, unlabeled_index = compute_train_sets(X_train, y_train,\n",
    "                                                                                          labeled_index,\n",
    "                                                                                          unlabeled_index, weights,\n",
    "                                                                                          iteration)\n",
    "    # (3) Training\n",
    "    history = model.fit(X_labeled_train, y_labeled_train, batch_size=2, nb_epoch=nb_active_epochs, verbose=1,\n",
    "                        shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "    log(history, iteration, log_file)\n",
    "    model.save(global_path + \"models/active_model\" + str(iteration) + \".h5\")\n",
    "\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e174c0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
